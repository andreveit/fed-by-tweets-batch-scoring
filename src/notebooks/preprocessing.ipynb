{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/andre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "from enelvo.normaliser import Normaliser\n",
    "from collections import Counter\n",
    "import spacy\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "import langdetect\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 80)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = set(stopwords.words('portuguese'))\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tokenize.casual.TweetTokenizer"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "type(TweetTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language(s):\n",
    "    try:\n",
    "        return langdetect.detect(s)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treat Users\n",
    "def treat_users(tweet_tkz, frequent_users):\n",
    "    '''\n",
    "    Apply functions to treat users\n",
    "    '''\n",
    "    tweet_tkz = remove_ats_frequet_users(tweet_tkz, frequent_users)\n",
    "    return remove_users(tweet_tkz)\n",
    "\n",
    "\n",
    "\n",
    "def get_users_rank(corpus, limit = 40):\n",
    "    '''\n",
    "    Returns rank (list of tuples) of most the frequent users in the corpus\n",
    "    '''\n",
    "    frame = pd.DataFrame({'corpus': corpus})\n",
    "    frame = frame[frame.corpus.str.contains('@')]\n",
    "    frame['at_'] = frame.corpus.apply(lambda doc: [word for word in doc.split() if '@' in word])\n",
    "    flatten = [num for elem in frame.at_.tolist() for num in elem]\n",
    "    rank = Counter(flatten).most_common(limit)\n",
    "    \n",
    "    return rank\n",
    "\n",
    "\n",
    "\n",
    "def get_frequent_users(corpus, limit=40):\n",
    "    rank = get_users_rank(corpus, limit)\n",
    "    return [i for i,j in rank]\n",
    "\n",
    "\n",
    "\n",
    "def remove_ats_frequet_users(tweet_tkz, frequent_users):\n",
    "    '''\n",
    "    Remove ats from frequent users, so they are kept in the corpus.\n",
    "    '''\n",
    "    intersection_set = set(tweet_tkz).intersection(set(frequent_users))\n",
    "    if len(intersection_set) != 0 :\n",
    "        for user in intersection_set:\n",
    "            tweet_tkz = [token.replace('@','')  if token == user else token for token in tweet_tkz]\n",
    "    return tweet_tkz\n",
    "\n",
    "\n",
    "def remove_users(tweet_tkz):\n",
    "    '''\n",
    "    Remove users with @.\n",
    "    '''\n",
    "    tweet = re.sub('@[^\\s]+','user',' '.join(tweet_tkz))\n",
    "    tweet_tkz = [token for token in tweet.split(' ') if token != 'user']\n",
    "    return tweet_tkz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Processing Emojis\n",
    "\n",
    "def remove_brs(tweet_tkz, replacement = 'brasil'):\n",
    "    '''\n",
    "    After tokenazation 🇧🇷 are turned into '🇧', '🇷'. Replacing them for \"brasil\".\n",
    "    '''\n",
    "    for i, _ in enumerate(tweet_tkz):\n",
    "        if tweet_tkz[i] in ('🇧','🇷'):\n",
    "            if i+1 <= len(tweet_tkz):\n",
    "                if tweet_tkz[i+1] in ('🇧','🇷'):\n",
    "                    tweet_tkz[i] = replacement\n",
    "                    tweet_tkz[i+1] = ''\n",
    "    tweet_tkz = [i for i in tweet_tkz if i != '']\n",
    "    return tweet_tkz\n",
    "\n",
    "\n",
    "# Emojis\n",
    "\n",
    "\n",
    "def treat_emojis(tweet_tkz, top_rank_emjs):\n",
    "    '''\n",
    "    Replaces or removes emojis from tokenized tweet.\n",
    "    The result depends on the frequency emojis rank and available translations. \n",
    "    '''\n",
    "    all_emjs = emoji.UNICODE_EMOJI['pt'] \n",
    "\n",
    "    new_tweet_tkz = []\n",
    "    for token in tweet_tkz:\n",
    "        if token in top_rank_emjs:\n",
    "            new_tweet_tkz.append(top_rank_emjs[token])\n",
    "        elif token in all_emjs:\n",
    "\n",
    "            pass\n",
    "        else:\n",
    "            new_tweet_tkz.append(token)\n",
    "    return new_tweet_tkz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_top_rank_emjs(corpus, limit=40, filename='../../data/3-preprocessed/emojis_dict.json'):\n",
    "    '''\n",
    "    Return translation dictionary for the most frequent emojis\n",
    "    '''\n",
    "    emojs_translation = get_emojis_words(corpus, limit, filename)\n",
    "    top_rank_emjs = [i for i, _ in get_emojis_rank(corpus)]\n",
    "    return { k:v for k,v in  emojs_translation.items() if k in top_rank_emjs} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_emojis_words(corpus, limit=40, filename='../../data/3-preprocessed/emojis_dict.json'):\n",
    "    '''\n",
    "    Return a dict mapping emojis and their respective word to be replaced in the documents.\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "        translations = json.load(file)\n",
    "    \n",
    "    emjs_dict = emoji.UNICODE_EMOJI['pt'] \n",
    "\n",
    "    return { emoji: translations.get(item.replace(':',''))\n",
    "            for emoji, item in emjs_dict.items() \n",
    "            if translations.get(item.replace(':','')) is not None \n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_emojis_rank(corpus, limit=40):\n",
    "    '''\n",
    "    Rank the most frequent emotions in the corpus.\n",
    "    Return a list of tuples.\n",
    "    '''\n",
    "    list_of_lists = [extract_emojis(token) for token in corpus if not isinstance(extract_emojis(token), float)]\n",
    "    flat_list = [item for sublist in list_of_lists for item in sublist]\n",
    "    c = Counter(flat_list)\n",
    "    return c.most_common(limit)\n",
    "\n",
    "\n",
    "def extract_emojis(tweet):\n",
    "    '''\n",
    "    Return a list of emotions in a tweet.\n",
    "    '''\n",
    "    emjs_dict = emoji.UNICODE_EMOJI['pt'] \n",
    "    emjs = []\n",
    "    tweet_tkz = tknzr.tokenize(tweet)\n",
    "    for token in tweet_tkz:\n",
    "        if token in emjs_dict:\n",
    "            emjs.append(token)\n",
    "    if len(emjs)==0:\n",
    "        return np.nan\n",
    "    return emjs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_stopwords(tweet_tkz, filename = '../../data/3-preprocessed/stopwords.json'):\n",
    "    ''' \n",
    "    Load stopwords list from file and remove them from tokenized tweets.\n",
    "    '''\n",
    "    with open(filename, 'r') as file:\n",
    "        stopwords = json.load(file)\n",
    "        \n",
    "    stopwords = nltk_stopwords.union(set(stopwords))\n",
    "    return [token for token in tweet_tkz if token.lower() not in stopwords]\n",
    "\n",
    "\n",
    "\n",
    "def treat_kkk(tweet_tkz):\n",
    "    ''' \n",
    "    Replace laugh kkkk for \"risada\".\n",
    "    '''\n",
    "    return [ 'risada'  if token in ['kkk'+i*'k' for i in range(25)] else token for token in tweet ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split joined words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_joined_words(token):\n",
    "    '''\n",
    "    Rertuns a list of splited tokens if splitable.\n",
    "    '''\n",
    "    if not token.isupper():\n",
    "        return re.sub( r\"([A-Z])\", r\" \\1\", token).split()\n",
    "\n",
    "    return [token]\n",
    "\n",
    "\n",
    "def tokenize_joined_words(tweet_tkz):\n",
    "    ''' \n",
    "    Tokenize words without spaces.\n",
    "    '''\n",
    "    new_tweet_tkz = []\n",
    "    for token in tweet_tkz:\n",
    "        token = split_joined_words(token)\n",
    "        for sub_token in token:\n",
    "            new_tweet_tkz.append(sub_token)\n",
    "    \n",
    "    return new_tweet_tkz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test1': 1, 'test2': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('test.json', 'r') as file:\n",
    "    mydict = json.load(file)\n",
    "\n",
    "mydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSING WHOLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "# Load Data\n",
    "\n",
    "reload_data = False\n",
    "\n",
    "if reload_data:\n",
    "    twt = pd.read_parquet('../../data/1-raw/tweets.parquet')\n",
    "    places = pd.read_parquet('../../data/1-raw/places.parquet')\n",
    "    users = pd.read_parquet('../../data/1-raw/users.parquet')\n",
    "\n",
    "\n",
    "    twt['is_retweet'] = np.where(twt.text.str.contains('RT @'),1,0)\n",
    "    twt['created_at_date'] = twt.created_at.dt.date\n",
    "    twt['created_at_time'] = twt.created_at.dt.time\n",
    "\n",
    "    twt['emoji'] = twt.text.apply(lambda x: extract_emojis(x))\n",
    "    twt.emoji = twt.emoji.apply(lambda x: ''.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    twt['lang'] = twt.text.apply(lambda x: get_language(x))\n",
    "    twt.to_pickle('../../data/2-intermediate/tweets.pkl')\n",
    "\n",
    "else:\n",
    "    twt = pd.read_pickle('../../data/2-intermediate/tweets.pkl')\n",
    "    df = twt[twt.is_retweet == 0]\n",
    "    df = df.drop(columns=['import_date','file_name'])\n",
    "    df = df[df.lang=='pt']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# corpus = df.text.tolist()\n",
    "# top_rank_emjs = get_top_rank_emjs(corpus)\n",
    "# frequent_users = get_frequent_users(corpus)\n",
    "\n",
    "# norm = Normaliser(tokenizer='readable')\n",
    "# spc = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "# bag = []\n",
    "# for i, tweet in enumerate(corpus):\n",
    "#     clear_output(wait=True)\n",
    "#     print(round(i/len(corpus)*100,2), \" %\")\n",
    "\n",
    "#     tknzr = TweetTokenizer()\n",
    "\n",
    "#     tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',tweet)\n",
    "\n",
    "#     tweet = tweet.replace('R$','reais').replace('-','').replace('url','')\n",
    "#     tweet = tknzr.tokenize(tweet)\n",
    "#     tweet = treat_users(tweet, frequent_users)\n",
    "\n",
    "#     tweet = treat_emojis(tweet, top_rank_emjs)\n",
    "\n",
    "#     tweet = remove_brs(tweet)\n",
    "\n",
    "#     tweet = remove_stopwords(tweet)\n",
    "\n",
    "#     tweet = re.sub(r'[^\\w\\s]','',' '.join(tweet)).split(' ') # Usar por ultimo pois retira emojis\n",
    "#     tweet = [token for token in tweet if token != ''] # remove empty strings\n",
    "\n",
    "#     tweet = [token.lemma_.lower() for token in spc(' '.join(tweet))]\n",
    "    \n",
    "#     tweet = treat_kkk(tweet)\n",
    "\n",
    "#     tweet = tokenize_joined_words(tweet)\n",
    "\n",
    "#     tweet = norm.normalise(' '.join([token.lower() for token in tweet])).split(' ')\n",
    "\n",
    "#     tweet = remove_stopwords(tweet)\n",
    "\n",
    "#     tweet = [token if token != 'suboficial' else 'lulaoficial' for token in tweet ]\n",
    "#     tweet = [token if token != 'vagar' else 'vagabundo' for token in tweet ]\n",
    "#     tweet = [token if token != 'firsar' else 'risada' for token in tweet ]\n",
    "\n",
    "#     bag.append(tweet)\n",
    "\n",
    "\n",
    "# df_processed = pd.DataFrame({'text':bag})\n",
    "# df_processed.to_pickle('../../data/2-intermediate/corpus.plk')\n",
    "\n",
    "# df_processed = pd.concat([df.reset_index(drop=True), df_processed.rename(columns={'text':'tokens'})], axis=1)\n",
    "# df_processed.to_parquet('../../data/2-intermediate/tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andre/Documents/scripts/fedbytweets-ml/fed-by-tweets-batch-scoring/src/notebooks/preprocessing.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andre/Documents/scripts/fedbytweets-ml/fed-by-tweets-batch-scoring/src/notebooks/preprocessing.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_processed\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_processed' is not defined"
     ]
    }
   ],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/andre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import Counter\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from enelvo.normaliser import Normaliser\n",
    "from IPython.display import clear_output\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "###################################################\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "\n",
    "class BaseTreater(ABC):\n",
    "    '''\n",
    "    Base class for Treaters\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TreaterMixIn:\n",
    "    '''\n",
    "    Implement common methods for Treaters\n",
    "    '''\n",
    "\n",
    "    BASE_PATH = './data/'\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_filename(var, default):\n",
    "        if var is None:\n",
    "            return TreaterMixIn.BASE_PATH + default\n",
    "        else:\n",
    "            return var\n",
    "\n",
    "    @staticmethod\n",
    "    def save(object, filename):\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(object, file)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                return json.load(file)\n",
    "        except IOError as e:\n",
    "            raise IOError('File not found, most likely the transformer has not been fitted.')\n",
    "\n",
    "\n",
    "\n",
    "class EmojisTreater(BaseTreater, TreaterMixIn):\n",
    "    '''\n",
    "    Emojis Treatment\n",
    "\n",
    "    Most frequent EMOJIS are translated to words, while the least frequent ones are discarted.\n",
    "\n",
    "    Params:\n",
    "        tokenizer = tokenizer object to be used to tokenization.\n",
    "        mapdict_rank_file = (str | None) - Json file mapping ranked emojis to words\n",
    "        mapdict_file = (str | None) - Json file containing all the translations available \n",
    "    '''\n",
    "\n",
    "    def __init__(self, tokenizer, mapdict_rank_file = None, mapdict_file=None):\n",
    "        self.fullemjs_dict = emoji.UNICODE_EMOJI['pt'] \n",
    "        self.tokenizer = tokenizer\n",
    "        self.rank = None\n",
    "        self.mapdict_rank_file = self.get_filename(mapdict_rank_file, 'mapdict_rank.json')\n",
    "        self.mapdict_file = self.get_filename(mapdict_file, 'emojis_map_dict.json')\n",
    "\n",
    "\n",
    "    def transform(self, tweet_tkz):\n",
    "        '''\n",
    "        Chain transformations.\n",
    "        '''\n",
    "        out = self._transform(tweet_tkz)\n",
    "        tweet_tkz = self.remove_brs(out)\n",
    "        return tweet_tkz\n",
    "\n",
    "\n",
    "    def _transform(self, tweet_tkz):\n",
    "        '''\n",
    "        Replaces or removes emojis from tokenized tweet.\n",
    "        The result depends on the frequency emojis rank and available translations. \n",
    "        '''\n",
    "        mapdict_rank = self.load(self.mapdict_rank_file)\n",
    "\n",
    "        new_tweet_tkz = []\n",
    "        for token in tweet_tkz:\n",
    "            if token in mapdict_rank:\n",
    "                new_tweet_tkz.append(mapdict_rank[token])\n",
    "            elif token in self.fullemjs_dict:\n",
    "\n",
    "                pass\n",
    "            else:\n",
    "                new_tweet_tkz.append(token)\n",
    "        return new_tweet_tkz\n",
    "\n",
    "\n",
    "    def remove_brs(self, tweet_tkz, replacement = 'brasil'):\n",
    "        '''\n",
    "        After tokenazation 🇧🇷 are turned into '🇧', '🇷'. Replacing them for \"brasil\".\n",
    "        '''\n",
    "        for i, _ in enumerate(tweet_tkz):\n",
    "            if tweet_tkz[i] in ('🇧','🇷'):\n",
    "                if i+1 <= len(tweet_tkz):\n",
    "                    if tweet_tkz[i+1] in ('🇧','🇷'):\n",
    "                        tweet_tkz[i] = replacement\n",
    "                        tweet_tkz[i+1] = ''\n",
    "        tweet_tkz = [i for i in tweet_tkz if i != '']\n",
    "        return tweet_tkz\n",
    "\n",
    "\n",
    "    def fit(self, corpus, limit=40):\n",
    "        '''\n",
    "        Return translation dictionary for the most frequent emojis\n",
    "        '''\n",
    "        mapdict = self.get_mapdict()\n",
    "        top_rank_emjs = [i for i, _ in self.get_rank(corpus, limit)]\n",
    "        mapdict_rank = { k:v for k,v in  mapdict.items() if k in top_rank_emjs} \n",
    "\n",
    "        self.save(mapdict_rank, self.mapdict_rank_file)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_mapdict(self):\n",
    "        '''\n",
    "        Return a emojis dictionary mapping and their respective word to be replaced in the documents.\n",
    "        '''\n",
    "        mapdict = self.load(self.mapdict_file)\n",
    "\n",
    "        return { emoji: mapdict.get(item.replace(':',''))\n",
    "                for emoji, item in self.fullemjs_dict.items() \n",
    "                if mapdict.get(item.replace(':','')) is not None \n",
    "            }\n",
    "\n",
    "\n",
    "    def get_rank(self, corpus, limit=40):\n",
    "        '''\n",
    "        Rank the most frequent emotions in the corpus.\n",
    "        Returns a list of tuples.\n",
    "        '''\n",
    "        list_of_lists = [self._extract_emojis(token)\n",
    "                        for token in corpus \n",
    "                        if not isinstance(self._extract_emojis(token), float)]\n",
    "\n",
    "        flat_list = [item for sublist in list_of_lists for item in sublist]\n",
    "        c = Counter(flat_list)\n",
    "        return c.most_common(limit)\n",
    "\n",
    "\n",
    "    def _extract_emojis(self, tweet):\n",
    "        '''\n",
    "        Return a list of emotions in a tweet.\n",
    "        '''\n",
    "        emjs_dict = emoji.UNICODE_EMOJI['pt'] \n",
    "        emjs = []\n",
    "        tweet_tkz = self.tokenizer.tokenize(tweet)\n",
    "        for token in tweet_tkz:\n",
    "            if token in emjs_dict:\n",
    "                emjs.append(token)\n",
    "        if len(emjs)==0:\n",
    "            return np.nan\n",
    "        return emjs\n",
    "\n",
    "\n",
    "class LinkedWordsTreater(BaseTreater):\n",
    "    '''\n",
    "    Try to tokenze words without space\n",
    "    '''\n",
    "\n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweet_tkz):\n",
    "        ''' \n",
    "        Tokenize words without spaces.\n",
    "        '''\n",
    "        new_tweet_tkz = []\n",
    "        for token in tweet_tkz:\n",
    "            token = self._split_joined_words(token)\n",
    "            for sub_token in token:\n",
    "                new_tweet_tkz.append(sub_token)\n",
    "        \n",
    "        return new_tweet_tkz\n",
    "\n",
    "\n",
    "    def _split_joined_words(self, token):\n",
    "        '''\n",
    "        Rertuns a list of splited tokens, if splitable.\n",
    "        '''\n",
    "        if not token.isupper():\n",
    "            return re.sub( r\"([A-Z])\", r\" \\1\", token).split()\n",
    "\n",
    "        return [token]\n",
    "\n",
    "\n",
    "\n",
    "class UsersTreater(BaseTreater, TreaterMixIn):\n",
    "    '''\n",
    "    User Treatment\n",
    "\n",
    "    Most frequent users are kept, while the least frequent ones are discarted.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ranking_file = './users_ranking.json'):\n",
    "        self.ranking_file = ranking_file\n",
    "        self.rank = None\n",
    "\n",
    "    def fit(self, corpus, ranking_size = 40):\n",
    "        '''\n",
    "        Fit Treater to data.\n",
    "        '''\n",
    "        rank = self._get_rank(corpus, ranking_size)\n",
    "        self.rank = [i for i, _ in rank]\n",
    "        self.save(self.rank, self.ranking_file)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def _get_rank(self, corpus, ranking_size):\n",
    "        '''\n",
    "        Returns rank (list of tuples) of most the frequent users in the corpus\n",
    "        '''\n",
    "        frame = pd.DataFrame({'corpus': corpus})\n",
    "        frame = frame[frame.corpus.str.contains('@')]\n",
    "        frame['at_'] = frame.corpus.apply(lambda doc: [word for word in doc.split() if '@' in word])\n",
    "        flatten = [num for elem in frame.at_.tolist() for num in elem]\n",
    "        rank = Counter(flatten).most_common(ranking_size)\n",
    "        return rank  \n",
    "\n",
    "\n",
    "    def transform(self, tweet_tkz):\n",
    "        '''\n",
    "        Apply functions to treat users\n",
    "        '''\n",
    "        if self.rank is None:\n",
    "            self.rank = self.load(self.ranking_file)\n",
    "\n",
    "        tweet_tkz = self._remove_ats_frequet_users(tweet_tkz)\n",
    "        return self._remove_users(tweet_tkz)\n",
    "\n",
    "\n",
    "\n",
    "    def _remove_ats_frequet_users(self, tweet_tkz):\n",
    "        '''\n",
    "        Remove ats from frequent users, so they are kept in the corpus.\n",
    "        '''\n",
    "        intersection_set = set(tweet_tkz).intersection(set(self.rank))\n",
    "        if len(intersection_set) != 0 :\n",
    "            for user in intersection_set:\n",
    "                tweet_tkz = [token.replace('@','')  if token == user else token for token in tweet_tkz]\n",
    "        return tweet_tkz\n",
    "\n",
    "\n",
    "    def _remove_users(self, tweet_tkz):\n",
    "        '''\n",
    "        Remove users with @.\n",
    "        '''\n",
    "        tweet = re.sub('@[^\\s]+','user',' '.join(tweet_tkz))\n",
    "        tweet_tkz = [token for token in tweet.split(' ') if token != 'user']\n",
    "        return tweet_tkz\n",
    "\n",
    "\n",
    "\n",
    "class StopwordsTreater(BaseTreater, TreaterMixIn):\n",
    "    '''Remove stopwords'''\n",
    "\n",
    "    def __init__(self, filename = None):\n",
    "        self.stopwords = self.get_stopwords(filename)\n",
    "\n",
    "    def get_stopwords(self, filename):\n",
    "        filename = self.get_filename(filename, 'stopwords.json')\n",
    "        custom_stopwords = self.load(filename)\n",
    "        nltk_stopwords = set(stopwords.words('portuguese'))\n",
    "        return nltk_stopwords.union(set(custom_stopwords))\n",
    "\n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweet_tkz):\n",
    "        return [token for token in tweet_tkz if token.lower() not in self.stopwords]\n",
    "\n",
    "\n",
    "\n",
    "def treat_kkk(tweet_tkz):\n",
    "    ''' \n",
    "    Replace laugh kkkk for \"risada\".\n",
    "    '''\n",
    "    return [ 'risada'  if token in ['kkk'+i*'k' for i in range(25)] else token for token in tweet_tkz ]\n",
    "\n",
    "\n",
    "\n",
    "class TweetsTextPreprocessor(BaseEstimator, TransformerMixin, TreaterMixIn):\n",
    "    '''\n",
    "    Custom sklearn transformer to preprocess tweets text.\n",
    "    '''\n",
    "    def __init__(self, tokenizer = TweetTokenizer, verbose = False):\n",
    "        self.tokenizer = tokenizer()\n",
    "        self.verbose = verbose\n",
    "        self.norm = Normaliser(tokenizer='readable')\n",
    "        self.spc = spacy.load(\"pt_core_news_sm\")\n",
    "        self.treat_users = UsersTreater()\n",
    "        self.treat_emojis = EmojisTreater(self.tokenizer)\n",
    "        self.treat_stopwords = StopwordsTreater()\n",
    "        self.tokenize_joined_words = LinkedWordsTreater()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        \n",
    "        bag = []\n",
    "        for i, tweet in enumerate(X):\n",
    "            if self.verbose:\n",
    "                clear_output(wait=True)\n",
    "                print(round(i/len(X)*100,2), \" %\")\n",
    "\n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',tweet)\n",
    "            tweet = tweet.replace('R$','reais').replace('-','').replace('url','')\n",
    "            \n",
    "            tweet = self.tokenizer.tokenize(tweet)      # Tokenizing\n",
    "            \n",
    "            tweet = self.treat_users.transform(tweet)\n",
    "            tweet = self.treat_emojis.transform(tweet)\n",
    "            tweet = self.treat_stopwords.transform(tweet)\n",
    "\n",
    "            tweet = re.sub(r'[^\\w\\s]','',' '.join(tweet)).split(' ')    # Clean up symbols and punctuation\n",
    "            tweet = [token for token in tweet if token != '']           # Remove empty strings\n",
    "\n",
    "            tweet = [token.lemma_.lower() for token in self.spc(' '.join(tweet))]   # Lematizing\n",
    "            tweet = treat_kkk(tweet)\n",
    "            tweet = self.tokenize_joined_words.transform(tweet)\n",
    "            # tweet = self.norm.normalise(' '.join([token.lower() for token in tweet])).split(' ') # Normalizer\n",
    "            tweet = self.treat_stopwords.transform(tweet)\n",
    "\n",
    "            # Special corrections\n",
    "            tweet = [token if token != 'suboficial' else 'lulaoficial' for token in tweet ]\n",
    "            tweet = [token if token != 'vagar' else 'vagabundo' for token in tweet ]\n",
    "            tweet = [token if token != 'firsar' else 'risada' for token in tweet ]\n",
    "\n",
    "\n",
    "            if tweet == []:\n",
    "                tweet = ['null']\n",
    "            bag.append(' '.join(tweet))\n",
    "\n",
    "        return pd.DataFrame({'text':bag})\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.98  %\n"
     ]
    }
   ],
   "source": [
    "ntweets = 5000\n",
    "\n",
    "twt = pd.read_pickle('../../data/2-intermediate/tweets.pkl')\n",
    "df = twt[twt.is_retweet == 0]\n",
    "df = df.drop(columns=['import_date','file_name'])\n",
    "df = df[df.lang=='pt']\n",
    "\n",
    "transformer = TweetsTextPreprocessor(verbose=True)\n",
    "transformed = transformer.transform(df.iloc[:ntweets].text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bolsonaro junto</td>\n",
       "      <td>[bolsonaro, junto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toyjo desde início governo bolsonaro brasil re...</td>\n",
       "      <td>[tojo, desde, início, governo, bolsonaro, bras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aqui governo roubalheirar corrupção governo bo...</td>\n",
       "      <td>[aqui, governo, roubalheira, corrupção, govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desmentindo bolsonaro serviço utilidade pública</td>\n",
       "      <td>[desmentindo, bolsonaro, serviço, utilidade, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uolnoticia engraçar bolsominions justificar at...</td>\n",
       "      <td>[monotipia, achar, engraçar, homofóbicos, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nojento asqueroso desgoverno bolsonaro contra ...</td>\n",
       "      <td>[nojento, asqueroso, desgoverno, bolsonaro, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jornaleco imundo moral comprar dinheiro públic...</td>\n",
       "      <td>[jornaleco, imundo, moral, comprar, dinheiro, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deputadofederal primeiro turno folgar bolsonar...</td>\n",
       "      <td>[deputadofederal, primeiro, turno, folgar, bol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vivo presidente jair bolsonaro recepciona jorn...</td>\n",
       "      <td>[vivo, presidente, jair, bolsonaro, recepciona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cármen lúcia gravidade escândalo mec mandar pg...</td>\n",
       "      <td>[cármen, lúcia, gravidade, escândalo, mec, man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>investigação contra pedro guimarães assédio se...</td>\n",
       "      <td>[investigação, contra, pedro, guimarães, asséd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bolsonaro</td>\n",
       "      <td>[bolsonaro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vergonha poder escolher bolsonaro apuraçãoasse...</td>\n",
       "      <td>[vergonha, poder, escolher, bolsonaro, apuraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>provar presidente bolsonaro sempre lado popula...</td>\n",
       "      <td>[provar, presidente, bolsonaro, sempre, lado, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jairbolsonaro poder tentar surfar onda bolsona...</td>\n",
       "      <td>[jairbolsonaro, poder, tentar, pescar, onda, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>viajar bom bolsonaro collor arthur lira reeleg...</td>\n",
       "      <td>[viajar, bom, bolsonaro, collor, arthur, lira,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bolsonaro assediador</td>\n",
       "      <td>[bolsonaro, assedia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>proliferação falso profeta chantagiset achacad...</td>\n",
       "      <td>[proliferação, falso, profeta, chantagista, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>romeu dois obsessão bolsonaro ídolo fernando p...</td>\n",
       "      <td>[romeu, dois, obsessão, bolsonaro, ídolo, fern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pedro guimarães reuniãor funcionário agronegóc...</td>\n",
       "      <td>[pedro, guimarães, reunião, funcionário, agron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>agricultores holandês irritar após decisão pol...</td>\n",
       "      <td>[agricultores, holandês, irritar, após, decisã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sempre perguntar crente capaz cair conto vigár...</td>\n",
       "      <td>[sempre, perguntar, crente, capaz, cair, conto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>senadorhumberto negativo senador nordeste bols...</td>\n",
       "      <td>[senadorhumberto, negativo, senador, nordeste,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pandemia guerrar governo bolsonaro continuar h...</td>\n",
       "      <td>[pandemia, guerrear, governo, bolsonaro, conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bolsonaro continuar governo homem bem preciso ...</td>\n",
       "      <td>[bolsonaro, continuar, governo, homem, bem, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>uolnoticia vcs uolixo mto ridículos noção mili...</td>\n",
       "      <td>[monotipia, prolixo, ridículos, noção, militan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pão êpa definição perfazer chapa bolsonarobrag...</td>\n",
       "      <td>[pão, epa, definição, perfazer, chapa, bolsona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>matéria bem estilo imprensa marron bolsonaro a...</td>\n",
       "      <td>[matéria, bem, estilo, imprensa, marrons, bols...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bom dia maioria população recife aprovar gestã...</td>\n",
       "      <td>[bom, dia, maioria, população, recife, aprovar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bolsonaro sancionar lei devolver pis cofins co...</td>\n",
       "      <td>[bolsonaro, sancionar, lei, devolver, abono, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>taoquei1 uai lula sempre bolsonaro</td>\n",
       "      <td>[taoquei1, uai, lula, sempre, bolsonaro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>coadunando barbárie desembargador salles rossi</td>\n",
       "      <td>[coadunando, barbárie, desembargador, salles, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>plot twist barbie agente duplo célula fascista...</td>\n",
       "      <td>[plot, twist, barbie, agente, duplo, célula, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>análise fraco qnd lula sair disputa bolsonaro ...</td>\n",
       "      <td>[análise, fraco, lula, sair, disputa, bolsonar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>presidente bolsonaro</td>\n",
       "      <td>[presidente, bolsonaro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pena acontecendo esquerda fdp presidente reele...</td>\n",
       "      <td>[pena, acontecendo, esquerda, filho, puta, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>cabra mentiroso bostanaro bora seguir pessoal ...</td>\n",
       "      <td>[cabra, mentiroso, costaneiro, embora, seguir,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>desmentindo bolsonaro missão chegar criatura s...</td>\n",
       "      <td>[desmentindo, bolsonaro, missão, chegar, criat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>semana deltan dallagnol contra corrupção afirm...</td>\n",
       "      <td>[semana, silas, dirceu, contra, corrupção, afi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>preparemse presidente cef amigão bolsonaro acu...</td>\n",
       "      <td>[preparem-se, presidente, cef, amigão, bolsona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>brasil amadores governo bolsonaro escancarar v...</td>\n",
       "      <td>[brasil, amadores, governo, bolsonaro, desmasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>defendendo assédio sexual vc mulher dever exem...</td>\n",
       "      <td>[defendendo, assédio, sexual, mulher, dever, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bolsonaro receber principal âncora fox news tu...</td>\n",
       "      <td>[bolsonaro, receber, principal, âncora, fox, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bolsonaro 25 pessoa lunatico q apoiar nao impo...</td>\n",
       "      <td>[bolsonaro, 25, pessoa, lunático, apoiar, impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>verdade sobre bolsonaro verdade povo matar</td>\n",
       "      <td>[verdade, sobre, bolsonaro, verdade, povo, matar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>empoderamento feminino gov bolsonaro risada</td>\n",
       "      <td>[feminismo, feminino, governo, bolsonaro, risada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>null</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>perceber gar brasil descer ladeira estado mina...</td>\n",
       "      <td>[perceber, agar, brasil, descer, ladeira, esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4 voto 1 tjsp manter condenação bolsonaro ofen...</td>\n",
       "      <td>[4, voto, 1, tesa, manter, condenação, bolsona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>infelizmente degradaçãor sentido liberdade val...</td>\n",
       "      <td>[infelizmente, degradação, sentido, liberdade,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0                                     bolsonaro junto   \n",
       "1   toyjo desde início governo bolsonaro brasil re...   \n",
       "2   aqui governo roubalheirar corrupção governo bo...   \n",
       "3     desmentindo bolsonaro serviço utilidade pública   \n",
       "4   uolnoticia engraçar bolsominions justificar at...   \n",
       "5   nojento asqueroso desgoverno bolsonaro contra ...   \n",
       "6   jornaleco imundo moral comprar dinheiro públic...   \n",
       "7   deputadofederal primeiro turno folgar bolsonar...   \n",
       "8   vivo presidente jair bolsonaro recepciona jorn...   \n",
       "9   cármen lúcia gravidade escândalo mec mandar pg...   \n",
       "10  investigação contra pedro guimarães assédio se...   \n",
       "11                                          bolsonaro   \n",
       "12  vergonha poder escolher bolsonaro apuraçãoasse...   \n",
       "13  provar presidente bolsonaro sempre lado popula...   \n",
       "14  jairbolsonaro poder tentar surfar onda bolsona...   \n",
       "15  viajar bom bolsonaro collor arthur lira reeleg...   \n",
       "16                               bolsonaro assediador   \n",
       "17  proliferação falso profeta chantagiset achacad...   \n",
       "18  romeu dois obsessão bolsonaro ídolo fernando p...   \n",
       "19  pedro guimarães reuniãor funcionário agronegóc...   \n",
       "20  agricultores holandês irritar após decisão pol...   \n",
       "21  sempre perguntar crente capaz cair conto vigár...   \n",
       "22  senadorhumberto negativo senador nordeste bols...   \n",
       "23  pandemia guerrar governo bolsonaro continuar h...   \n",
       "24  bolsonaro continuar governo homem bem preciso ...   \n",
       "25  uolnoticia vcs uolixo mto ridículos noção mili...   \n",
       "26  pão êpa definição perfazer chapa bolsonarobrag...   \n",
       "27  matéria bem estilo imprensa marron bolsonaro a...   \n",
       "28  bom dia maioria população recife aprovar gestã...   \n",
       "29  bolsonaro sancionar lei devolver pis cofins co...   \n",
       "30                 taoquei1 uai lula sempre bolsonaro   \n",
       "31     coadunando barbárie desembargador salles rossi   \n",
       "32  plot twist barbie agente duplo célula fascista...   \n",
       "33  análise fraco qnd lula sair disputa bolsonaro ...   \n",
       "34                               presidente bolsonaro   \n",
       "35  pena acontecendo esquerda fdp presidente reele...   \n",
       "36  cabra mentiroso bostanaro bora seguir pessoal ...   \n",
       "37  desmentindo bolsonaro missão chegar criatura s...   \n",
       "38  semana deltan dallagnol contra corrupção afirm...   \n",
       "39  preparemse presidente cef amigão bolsonaro acu...   \n",
       "40  brasil amadores governo bolsonaro escancarar v...   \n",
       "41  defendendo assédio sexual vc mulher dever exem...   \n",
       "42  bolsonaro receber principal âncora fox news tu...   \n",
       "43  bolsonaro 25 pessoa lunatico q apoiar nao impo...   \n",
       "44         verdade sobre bolsonaro verdade povo matar   \n",
       "45        empoderamento feminino gov bolsonaro risada   \n",
       "46                                               null   \n",
       "47  perceber gar brasil descer ladeira estado mina...   \n",
       "48  4 voto 1 tjsp manter condenação bolsonaro ofen...   \n",
       "49  infelizmente degradaçãor sentido liberdade val...   \n",
       "\n",
       "                                               tokens  \n",
       "0                                  [bolsonaro, junto]  \n",
       "1   [tojo, desde, início, governo, bolsonaro, bras...  \n",
       "2   [aqui, governo, roubalheira, corrupção, govern...  \n",
       "3   [desmentindo, bolsonaro, serviço, utilidade, p...  \n",
       "4   [monotipia, achar, engraçar, homofóbicos, just...  \n",
       "5   [nojento, asqueroso, desgoverno, bolsonaro, co...  \n",
       "6   [jornaleco, imundo, moral, comprar, dinheiro, ...  \n",
       "7   [deputadofederal, primeiro, turno, folgar, bol...  \n",
       "8   [vivo, presidente, jair, bolsonaro, recepciona...  \n",
       "9   [cármen, lúcia, gravidade, escândalo, mec, man...  \n",
       "10  [investigação, contra, pedro, guimarães, asséd...  \n",
       "11                                        [bolsonaro]  \n",
       "12  [vergonha, poder, escolher, bolsonaro, apuraçã...  \n",
       "13  [provar, presidente, bolsonaro, sempre, lado, ...  \n",
       "14  [jairbolsonaro, poder, tentar, pescar, onda, b...  \n",
       "15  [viajar, bom, bolsonaro, collor, arthur, lira,...  \n",
       "16                               [bolsonaro, assedia]  \n",
       "17  [proliferação, falso, profeta, chantagista, ac...  \n",
       "18  [romeu, dois, obsessão, bolsonaro, ídolo, fern...  \n",
       "19  [pedro, guimarães, reunião, funcionário, agron...  \n",
       "20  [agricultores, holandês, irritar, após, decisã...  \n",
       "21  [sempre, perguntar, crente, capaz, cair, conto...  \n",
       "22  [senadorhumberto, negativo, senador, nordeste,...  \n",
       "23  [pandemia, guerrear, governo, bolsonaro, conti...  \n",
       "24  [bolsonaro, continuar, governo, homem, bem, pr...  \n",
       "25  [monotipia, prolixo, ridículos, noção, militan...  \n",
       "26  [pão, epa, definição, perfazer, chapa, bolsona...  \n",
       "27  [matéria, bem, estilo, imprensa, marrons, bols...  \n",
       "28  [bom, dia, maioria, população, recife, aprovar...  \n",
       "29  [bolsonaro, sancionar, lei, devolver, abono, t...  \n",
       "30           [taoquei1, uai, lula, sempre, bolsonaro]  \n",
       "31  [coadunando, barbárie, desembargador, salles, ...  \n",
       "32  [plot, twist, barbie, agente, duplo, célula, f...  \n",
       "33  [análise, fraco, lula, sair, disputa, bolsonar...  \n",
       "34                            [presidente, bolsonaro]  \n",
       "35  [pena, acontecendo, esquerda, filho, puta, pre...  \n",
       "36  [cabra, mentiroso, costaneiro, embora, seguir,...  \n",
       "37  [desmentindo, bolsonaro, missão, chegar, criat...  \n",
       "38  [semana, silas, dirceu, contra, corrupção, afi...  \n",
       "39  [preparem-se, presidente, cef, amigão, bolsona...  \n",
       "40  [brasil, amadores, governo, bolsonaro, desmasc...  \n",
       "41  [defendendo, assédio, sexual, mulher, dever, e...  \n",
       "42  [bolsonaro, receber, principal, âncora, fox, n...  \n",
       "43  [bolsonaro, 25, pessoa, lunático, apoiar, impo...  \n",
       "44  [verdade, sobre, bolsonaro, verdade, povo, matar]  \n",
       "45  [feminismo, feminino, governo, bolsonaro, risada]  \n",
       "46                                                 []  \n",
       "47  [perceber, agar, brasil, descer, ladeira, esta...  \n",
       "48  [4, voto, 1, tesa, manter, condenação, bolsona...  \n",
       "49  [infelizmente, degradação, sentido, liberdade,...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = pd.read_parquet('../../data/2-intermediate/tweets.parquet')\n",
    "\n",
    "df_ = pd.concat([transformed, df_processed.iloc[:ntweets].tokens.to_frame()], axis=1)\n",
    "# df_['test'] = np.where(df_.text == df_.tokens, 1, 0)\n",
    "# df_.test.sum()\n",
    "\n",
    "df_.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Os',\n",
       " '47',\n",
       " '%',\n",
       " 'do',\n",
       " 'Lula',\n",
       " 'Vivem',\n",
       " 'em',\n",
       " 'Marte',\n",
       " 'ou',\n",
       " 'na',\n",
       " 'Lua',\n",
       " ',',\n",
       " 'porqu',\n",
       " '...',\n",
       " '2208',\n",
       " 'Bom',\n",
       " 'dia',\n",
       " 'querides',\n",
       " '⭐',\n",
       " '🇻',\n",
       " '🇳']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "transformer = LinkedWordsTreater()\n",
    "# transformer = transformer.fit(df.text.tolist(), 40)\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "tweet = 'Os 47% do LulaVivem em Marte ou na Lua, porqu...2208    Bom dia querides ⭐🇻🇳'\n",
    "tweet = tknzr.tokenize(tweet)\n",
    "\n",
    "transformer.transform(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/andre/Documents/scripts/fedbytweets-ml/fed-by-tweets-batch-scoring/src/notebooks/preprocessing.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andre/Documents/scripts/fedbytweets-ml/fed-by-tweets-batch-scoring/src/notebooks/preprocessing.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andre/Documents/scripts/fedbytweets-ml/fed-by-tweets-batch-scoring/src/notebooks/preprocessing.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andre/Documents/scripts/fedbytweets-ml/fed-by-tweets-batch-scoring/src/notebooks/preprocessing.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\n",
    "from . import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/andre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac77d250cb3e323ca0c03edc048fddfb77bdf1ff531383eb4a0b861c607b7b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
